{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning for Custom Datasets in the Small-Data Regime for Basement\n",
    "\n",
    "# Milestone 1: Environment Preparation\n",
    "First step is to install a data anotation tool CVAT locally in our laptop.The document below cointains instruction for insalling on MAC OS with following system specifictions :\n",
    "|  |   |\n",
    "|--|--|\n",
    "| Processer | 6-Core Intel i7  |\n",
    "|Processer Speed|2.6 Ghz|\n",
    "|Memory|16GB|\n",
    "|System Version|macOS 12.4|\n",
    "|Kernal Version|Darwin 21.5.0|\n",
    "\n",
    "\n",
    "## Quick installation guide\n",
    "### Install and run Docker Desktop on Mac\n",
    "\n",
    " 1. Downloaded `Docker.dmg` from this [link](https://desktop.docker.com/mac/main/amd64/Docker.dmg?utm_source=docker&utm_medium=webreferral&utm_campaign=docs-driven-download-mac-amd64)\n",
    " 2. Double-click  `Docker.dmg`  to open the installer, then drag the Docker icon to the Applications folder.\n",
    " 3. Double-click  `Docker.app`  in the  **Applications**  folder to start Docker.\n",
    " 4. The Docker menu (![whale menu](https://docs.docker.com/desktop/install/images/whale-x.svg)) displays the Docker Subscription Service Agreement window. Select **Accept** to continue. \n",
    " 5. Docker Desktop starts after you accept the terms.\n",
    "\n",
    "### Install Git\n",
    "For me I already had `git` installed. If you donâ€™t have it installed already you can find instructions  [here](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).\n",
    "\n",
    "### Download and install Google Chrome\n",
    "Google Chrome is the only browser which is supported by CVAT.\n",
    "\n",
    " 1. Go to [Download Chrome](https://www.google.com/chrome/)\n",
    " 2. Click Download Chrome.\n",
    " 3. Double-click  `GoogleChrome.dmg`  to open the installer, then drag the Google Chrome icon to the Applications folder.\n",
    " 4. Double-click  `GoogleChrome.app`  in the  **Applications**  folder to start Google Chrome.\n",
    "\n",
    "### Installing CVAT\n",
    "\n",
    " 1. Open a terminl Window.\n",
    " 2. Clone  _CVAT_  source code from the  [GitHub repository](https://github.com/opencv/cvat)  with Git. The following command will clone the latest develop branch:\n",
    "  ```shell\n",
    "git clone https://github.com/opencv/cvat\n",
    "```\n",
    "3. Move into _CVAT_ folder .\n",
    "```shell\n",
    "cd cvat\n",
    "```\n",
    "4. Run docker containers. It will take some time to download the latest CVAT release and other required images like postgres, redis, etc. from DockerHub and create containers.\n",
    " ```shell\n",
    "    docker-compose up -d\n",
    "   ```\n",
    "5. You can register a user but by default it will not have rights even to view list of tasks. Thus you should create a superuser. A superuser can use an admin panel to assign correct groups to other users. Please use the command below:\n",
    "```shell\n",
    "docker exec -it cvat_server bash -ic 'python3 ~/manage.py createsuperuser'\n",
    "```\n",
    "6. Choose a username and a password for your admin account.\n",
    "7. Open the installed Google Chrome browser and go to [localhost:8080](http://localhost:8080/). Type your login/password for the superuser on the login page and press the _Login_ button. Now you should be able to create a new annotation task.\n",
    "#### Login Page\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.ibb.co/sC4QpPf/bfcb2be8-f9d6-4cd8-bad5-df1a83dbd076.jpg\" />\n",
    "</p>\n",
    "\n",
    "#### Home Page\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.ibb.co/VBTNJNH/ac3e0527-e58f-4928-bec7-69b0da1ab52b.jpg\" />\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning for Custom Datasets in the Small-Data Regime for Basement\n",
    "\n",
    "# Milestone 2: Data Acquisition\n",
    "\n",
    "For the basement I shortlisted following 10 product catogries that HD sells and are visible in their video :\n",
    "\n",
    " 1. Pillar Wood/Steel \n",
    " 2. Television \n",
    " 3. Photo Frame\n",
    " 4. Lamp and Lighting \n",
    " 5. Fireplace \n",
    " 6. Flooring \n",
    " 7. Sofa \n",
    " 8. Table \n",
    " 9. Installation Service \n",
    " 10. Window \n",
    "\n",
    "I used Google Custom Search API to get images for disered labels. Here is the code for a particular lable :\n",
    "```python\n",
    "# !pip install google-api-python-client\n",
    "\n",
    "import  requests\n",
    "from  PIL  import  Image\n",
    "import  os\n",
    "\n",
    "api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "from apiclient.discovery import build\n",
    "resource = build(\"customsearch\", 'v1', developerKey=api_key).cse()\n",
    "items = []\n",
    "\n",
    "search_string = \"basement wire\"\n",
    "cx = \"xxxxxxxxxxx\"  #Custom Search Engine ID\n",
    "\n",
    "for  rng  in  range(1,100,10):\n",
    "\tprint(rng, end=' ')\n",
    "\tresult = resource.list(q=search_string, cx='c027b0cbdfed84f0b', searchType = 'image',start=rng).execute()\n",
    "\tfor  item  in  result['items']:\n",
    "\t\titems.append([item['title'], item['link']])\n",
    "\n",
    "# Saving Images into folder\n",
    "dir_name = './wire_data/'\n",
    "if  dir_name  not  in  os.listdir():\n",
    "\tos.makedirs('dir_name')\n",
    "\t\n",
    "i=0\n",
    "for  item  in  items:\n",
    "\timg_url = item[1]\n",
    "\tresponse = requests.get(img_url)\n",
    "\tif  response.status_code:\n",
    "\t\tfp = open('./wire/+'+str(i)+'.png', 'wb')\n",
    "\t\tfp.write(response.content)\n",
    "\t\tfp.close()\n",
    "\tprint(i,item[1])\n",
    "\ti+=1\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning for Custom Datasets in the Small-Data Regime for Basement\n",
    "\n",
    "# Milestone 3: Annotation\n",
    "\n",
    "## **Deep Extreme Cut: From Extreme Points to Object Segmentation**\n",
    "\n",
    "DEXTR (Deep Extreme Cut) is a deep learning architecture that is designed to perform object segmentation in images. The architecture is based on the idea of \"extreme points,\" which are the points in an image that are farthest away from the background. By identifying these extreme points and using them as the starting points for object segmentation, DEXTR is able to more accurately segment objects in images.\n",
    "\n",
    "**![](https://lh6.googleusercontent.com/tcig1ZD6GH0imaEwYKfZbeBaTl0O1kdnBaEHtYsc8Qzn5CMGAi_poKhNtQDxk8R4Fu8Sg3x8ABQ28Ao5avU-IEMl7hn2ICrCvlNOhIMmS6ZyNZmRBfKIkeY0tRM87xPXsTrX-71Yr-yjQwvydCj-XK4yoRU-OEeAsS3mU7cCPTs2UmXtRAeBwMVVRZ3eHQ)**\n",
    "\n",
    "The architecture of DEXTR consists of two main components: a feature extraction network and a decoder network. The feature extraction network is typically a pre-trained deep convolutional neural network (CNN), such as VGG-16 or ResNet-101, that is used to extract high-level features from the input image. The decoder network, on the other hand, is a fully-connected network that takes the output of the feature extraction network and generates a segmentation map for the input image.\n",
    "\n",
    "  \n",
    "\n",
    "## Feature Extraction:\n",
    "\n",
    "  \n",
    "\n",
    "A 4-channel input for the CNN is created in the DEXTR architecture by concatenating the heatmap with the RGB channels of the input image. A bounding box created from the extreme point annotations is used to crop this input. The bounding box is then relaxed by a few pixels to include context surrounding the object of interest. As a result of this preprocessing step, the input is an RGB crop that includes the object and its extreme points.\n",
    "\n",
    "  \n",
    "\n",
    "The ResNet-101 is used to extract the features. The fully connected layer and the last 2 max pooling layer are removed to get the required resolution(atrous convolution). After the ResNet, a pyramid scene based parsing module is designed to collect globally valuable features. The weights of the network are pre-defined using ImageNet to give better results. The output of the feature extraction part is the probability map. It represents the probability of each pixel belonging to an object.\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "CNN tries to minimize the given cost function. The given cost function is called the entropy loss function between the label and the prediction $\\hat{y}_j$\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "$$\\mathcal{L}=\\sum_{j \\in Y} w_{Y_j} C\\left(y_j, \\hat{y}_j\\right), \\quad j \\in  1, \\ldots,|Y|$$\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "## Decoder Network:\n",
    "\n",
    "  \n",
    "\n",
    "To generate the segmentation map, the decoder network first identifies the extreme points in the input image using a set of predefined rules. These points are then used as the starting points for a graph-based segmentation algorithm, which is used to generate the final segmentation map. This map is then refined using a set of post-processing steps, such as removing small objects and filling in gaps in the segmentation, to produce the final segmentation map for the input image.\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "## Interactive Segmentation:\n",
    "\n",
    "  \n",
    "\n",
    "Interactive segmentation is a method of segmenting objects in images that involves providing user input to guide the segmentation process. In the context of the Deep Extreme Cut architecture, interactive segmentation may be performed by providing the model with \"extreme points\" as input, which are the points in an image that are farthest away from the background. These points are used as the starting points for the DEXTR model to segment the objects in the image, allowing the user to guide the segmentation process by selecting the points that are most relevant to the task at hand.\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "The interactive segmentation can improve the accuracy and efficiency of object segmentation by allowing the user to provide guidance to the model, and can be a useful tool in situations where the objects of interest are not well-defined or are difficult to segment automatically.\n",
    "\n",
    "  \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "  \n",
    "\n",
    "To conclude, the DEXTR architecture combines the strengths of both convolutional neural networks and graph-based methods to achieve state-of-the-art performance on a variety of object segmentation tasks. Due to this, there has been widespread adoption in the research community.\n",
    "\n",
    "\n",
    "# Annotation Link\n",
    "The dataset is uploaded on drive at this [Basement Data Set](https://drive.google.com/drive/folders/1bQI5NGNnWryRgPsNFFRrRT2tpAoFJZl2?usp=sharing)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning for Custom Datasets in the Small-Data Regime for Basement\n",
    "\n",
    "# Milestone 4: Semantic Segmentation\n",
    "![Sample Segementation](https://i.ibb.co/sH6FX64/6a745d3b-952d-49de-9946-fde2a6b024f1.jpg)\n",
    "\n",
    "The architecture was inspired by  [U-Net: CNN for Biomedical Image Segmentation](http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).\n",
    "## Network Description\n",
    "\n",
    "### Data\n",
    "\n",
    "The pretrained data is from ADK20k images dataset fine tuned for our scraped images to be found here [Basement Data Set](https://drive.google.com/drive/folders/1bQI5NGNnWryRgPsNFFRrRT2tpAoFJZl2?usp=sharing)\n",
    "\n",
    "### Model\n",
    "\n",
    "[![img/u-net-architecture.png](https://github.com/zhixuhao/unet/raw/master/img/u-net-architecture.png)](https://github.com/zhixuhao/unet/blob/master/img/u-net-architecture.png)\n",
    "\n",
    "This deep neural network is implemented with Keras functional API, which makes it extremely easy to experiment with different interesting architectures.\n",
    "\n",
    "Output from the network is a 512*512 which represents mask that should be learned. Sigmoid activation function makes sure that mask pixels are in [0, 1] range.\n",
    "\n",
    "### Training\n",
    "\n",
    "The model is trained for 5 epochs.\n",
    "\n",
    "After 5 epochs, calculated accuracy is about 0.97.\n",
    "\n",
    "Loss function for the training is basically just a binary crossentropy.\n",
    "\n",
    "## How to Use\n",
    "### Dependencies\n",
    "\n",
    "This tutorial depends on the following libraries:\n",
    "-   Hardware: >=4 GPUs for training, >=1 GPU for testing (set  `[--gpus GPUS]`  accordingly)\n",
    "-   Software: Ubuntu 16.04.3 LTS,  _**CUDA>=8.0, Python>=3.5, PyTorch>=0.4.0**_\n",
    "-   Dependencies: numpy, scipy, opencv, yacs, tqdm\n",
    "\n",
    "Also, this code should be compatible with Python versions 2.7-3.5.\n",
    "\n",
    "1.  Here is a simple demo to do inference on a single image:\n",
    "```\n",
    "chmod +x demo_test.sh\n",
    "./demo_test.sh\n",
    "```\n",
    "This script downloads a trained model (UNet50dilated) and a test image, runs the test script, and saves predicted segmentation (.png) to the working directory.\n",
    "\n",
    "2.  To test on an image or a folder of images (`$PATH_IMG`), you can simply do the following:\n",
    "\n",
    "```python\n",
    "python3 -u test.py --imgs $PATH_IMG --gpu $GPU --cfg $CFG\n",
    "\n",
    "```\n",
    "\n",
    "## Results \n",
    "\n",
    "### Preformance \n",
    "\n",
    "| Architecture | Mean IoU |Pixel Accuracy(%) |Overall Score |Inference Speed(fps) |\n",
    "|--|--|--|--|--|\n",
    "| UNet50dilated | 42.14 |80.13 |61.14 | 2.6|\n",
    "|UNet18dilated|38.00|78.64\t|58.32|11.7|\n",
    "\n",
    "### Image Output\n",
    "![enter image description here](https://i.ibb.co/t2ckyq1/1813462c-15a7-44a3-a0ed-421e9482ab64.jpg)\n",
    "\n",
    "![enter image description here](https://i.ibb.co/HdQpYH7/0e416353-5d97-454f-affa-9c8deb5563a2.jpg)\n",
    "\n",
    "![enter image description here](https://i.ibb.co/VNhHDGk/04a054c1-74b9-4339-9f59-edcbb3017ee6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
