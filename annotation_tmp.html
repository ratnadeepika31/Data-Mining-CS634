<!DOCTYPE html>
<html>
<head>
<title>annotation.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h3 id="transfer-learning-for-custom-datasets-in-the-small-data-regime-for-basement">Transfer Learning for Custom Datasets in the Small-Data Regime for Basement</h3>
<h1 id="milestone-3-annotation">Milestone 3: Annotation</h1>
<h2 id="deep-extreme-cut-from-extreme-points-to-object-segmentation"><strong>Deep Extreme Cut: From Extreme Points to Object Segmentation</strong></h2>
<p>DEXTR (Deep Extreme Cut) is a deep learning architecture that is designed to perform object segmentation in images. The architecture is based on the idea of &quot;extreme points,&quot; which are the points in an image that are farthest away from the background. By identifying these extreme points and using them as the starting points for object segmentation, DEXTR is able to more accurately segment objects in images.</p>
<p><strong><img src="https://lh6.googleusercontent.com/tcig1ZD6GH0imaEwYKfZbeBaTl0O1kdnBaEHtYsc8Qzn5CMGAi_poKhNtQDxk8R4Fu8Sg3x8ABQ28Ao5avU-IEMl7hn2ICrCvlNOhIMmS6ZyNZmRBfKIkeY0tRM87xPXsTrX-71Yr-yjQwvydCj-XK4yoRU-OEeAsS3mU7cCPTs2UmXtRAeBwMVVRZ3eHQ" alt=""></strong></p>
<p>The architecture of DEXTR consists of two main components: a feature extraction network and a decoder network. The feature extraction network is typically a pre-trained deep convolutional neural network (CNN), such as VGG-16 or ResNet-101, that is used to extract high-level features from the input image. The decoder network, on the other hand, is a fully-connected network that takes the output of the feature extraction network and generates a segmentation map for the input image.</p>
<h2 id="feature-extraction">Feature Extraction:</h2>
<p>A 4-channel input for the CNN is created in the DEXTR architecture by concatenating the heatmap with the RGB channels of the input image. A bounding box created from the extreme point annotations is used to crop this input. The bounding box is then relaxed by a few pixels to include context surrounding the object of interest. As a result of this preprocessing step, the input is an RGB crop that includes the object and its extreme points.</p>
<p>The ResNet-101 is used to extract the features. The fully connected layer and the last 2 max pooling layer are removed to get the required resolution(atrous convolution). After the ResNet, a pyramid scene based parsing module is designed to collect globally valuable features. The weights of the network are pre-defined using ImageNet to give better results. The output of the feature extraction part is the probability map. It represents the probability of each pixel belonging to an object.</p>
<p>CNN tries to minimize the given cost function. The given cost function is called the entropy loss function between the label and the prediction $\hat{y}_j$</p>
<p>$$\mathcal{L}=\sum_{j \in Y} w_{Y_j} C\left(y_j, \hat{y}_j\right), \quad j \in  1, \ldots,|Y|$$</p>
<h2 id="decoder-network">Decoder Network:</h2>
<p>To generate the segmentation map, the decoder network first identifies the extreme points in the input image using a set of predefined rules. These points are then used as the starting points for a graph-based segmentation algorithm, which is used to generate the final segmentation map. This map is then refined using a set of post-processing steps, such as removing small objects and filling in gaps in the segmentation, to produce the final segmentation map for the input image.</p>
<h2 id="interactive-segmentation">Interactive Segmentation:</h2>
<p>Interactive segmentation is a method of segmenting objects in images that involves providing user input to guide the segmentation process. In the context of the Deep Extreme Cut architecture, interactive segmentation may be performed by providing the model with &quot;extreme points&quot; as input, which are the points in an image that are farthest away from the background. These points are used as the starting points for the DEXTR model to segment the objects in the image, allowing the user to guide the segmentation process by selecting the points that are most relevant to the task at hand.</p>
<p>The interactive segmentation can improve the accuracy and efficiency of object segmentation by allowing the user to provide guidance to the model, and can be a useful tool in situations where the objects of interest are not well-defined or are difficult to segment automatically.</p>
<h2 id="conclusion">Conclusion</h2>
<p>To conclude, the DEXTR architecture combines the strengths of both convolutional neural networks and graph-based methods to achieve state-of-the-art performance on a variety of object segmentation tasks. Due to this, there has been widespread adoption in the research community.</p>
<h1 id="annotation-link">Annotation Link</h1>
<p>The dataset is uploaded on drive at this <a href="https://drive.google.com/drive/folders/1bQI5NGNnWryRgPsNFFRrRT2tpAoFJZl2?usp=sharing">Basement Data Set</a></p>

</body>
</html>
